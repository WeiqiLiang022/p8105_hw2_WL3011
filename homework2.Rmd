---
title: "p8105_hw2_WL3011"
author: "Weiqi Liang"
date: "`r Sys.Date()`"
output: github_document
always_allow_html: true
---

## Setup File
```{r setup, message = FALSE, results='hide'}
library(tidyverse)
library(dplyr)
library(readxl)
library(haven)
```

## I. Problem 1

### 1.1 Load the NYC Subway Dataset

*   Retained columns: `Line`, `Station Name`, `Station Latitude`, `Station Longitude`, `Route1`:`Route11`, `Entry`, `Vending`, `Entrance Type`, `ADA`;
*   Converted the `Entry` variable from "YES" and "NO" to logical TRUE and FALSE.
```{r}
# Load the NYC Subway csv
subway_df = 
  read.csv("./NYC_Transit_Subway_Entrance_And_Exit_Data.csv", 
           na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  select(line, station_name, station_latitude, station_longitude,
         route1:route11, entry, vending, entrance_type, ada) |>
  mutate(
    entry_logical = case_match(
      entry, 
      "YES" ~ TRUE,
      "NO"  ~ FALSE
      )
    )
```
The dataset now is tidy, with each row representing a station entrance and all columns having consistent data types. It has `r nrow(subway_df)` rows and `r ncol(subway_df)` columns, showing New York City subway station entrances/exits. The columns include:

**Station details**: Line, Station Name

**Routes**: Route1 to Route11, showing the subway lines served at the station

**Facilities**: Entrance Type, Entry, Vending, ADA (compliance), and ADA Notes

**Geographical details**: Entrance Latitude/Longitude, North/South and East/West Streets

### 1.2 Answering the Following Question
1.  How many distinct stations are there?
```{r results='hide'}
# calculate the number of distinct stations (identified by name and line)
distinct_stations = subway_df |>
  distinct(station_name, line) |>
  nrow()
```
There are `r distinct_stations` distinct stations, identified by their name and line.

2.  How many stations are ADA compliant?
```{r results='hide'}
# the number of ADA compliant stations
ada_compliant_stations = subway_df |>
  filter(ada == TRUE) |>
  distinct(station_name, line) |>
  nrow()
```
There are `r ada_compliant_stations` ADA compliant stations.

3.  What proportion of station entrances / exits without vending allow entrance?
```{r results='hide'}
no_vending_entry = subway_df |>
  filter(vending == "NO") |>
  filter(entry == "YES" ) |>
  nrow()
proportion = no_vending_entry/nrow(subway_df)
```
The proportion of station entrances without vending allow entrance is `r proportion`.

### 1.3 Reformat Dataset
1.  Reformat data so that route number and route name are distinct variables.
```{r}
# Split route to long format (pivot_longer)
Reformat_subway_df = subway_df |>
  mutate_at(vars(route1:route11), as.character) |>
  pivot_longer(
    cols = route1:route11, 
    names_to = "route_number", 
    values_to = "route_name"
    ) |>
  filter(!is.na(route_name))  # remove NA

head(Reformat_subway_df)
```

2.  How many distinct stations serve the A train? 
```{r results='hide'}
# number of stations which serve A train
a_train_stations = Reformat_subway_df |>
  filter(route_name == "A") |>
  distinct(station_name, line) |>
  nrow()
```
There are `r a_train_stations` stations serve the A train.

3.  Of the stations that serve the A train, how many are ADA compliant?
```{r results='hide'}
# number of ADA compliant stations which serve A train
a_train_stations_ADA = Reformat_subway_df |>
  filter(route_name == "A", ada == TRUE) |>
  distinct(station_name, line) |>
  nrow()
```
There are `r a_train_stations_ADA` ADA compliant stations serve the A train.

## II. Problem 2

### 2.1 Load the Mr. Trash Wheel Sheet
*   Import the  **Mr. Trash Wheel** sheet, while omitting non-data entries;
*   Omit rows that do not include dumpster-specific data;
*   Round the number of `sports_balls`.

```{r message=FALSE}
# Load the Trash Wheel xlsx
trash_wheel_path = "./202409 Trash Wheel Collection Data.xlsx"
MTW = 
  readxl::read_excel(trash_wheel_path, sheet = "Mr. Trash Wheel", 
                     skip = 1, na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  select(dumpster:homes_powered) |>
  drop_na(dumpster) |>
  mutate(
    sports_balls_integer = as.integer(round(sports_balls))
    )
```

Similarly, import the **Professor Trash Wheel** and **Gwynnda Trash Wheel** sheets.
```{r}
# Load the PTW and GTW sheet
PTW = 
  readxl::read_excel(trash_wheel_path, sheet = "Professor Trash Wheel", 
                     skip = 1, na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  select(dumpster:homes_powered) |>
  drop_na(dumpster, month) 

GTW = 
  readxl::read_excel(trash_wheel_path, sheet = "Gwynnda Trash Wheel", 
                     skip = 1, na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  select(dumpster:homes_powered) |>
  drop_na(dumpster, month) 

```

### 2.2 Combine PTW and GTW with MTW 
```{r}
MTW = MTW |>
  mutate(category = "Mr._Trash_Wheel") |>
  mutate(year = as.character(year))
PTW = PTW |>
  mutate(category = "Professor_Trash_Wheel") |>
  mutate(year = as.character(year))
GTW = GTW |>
  mutate(category = "Gwynnda_Trash_Wheel") |>
  mutate(year = as.character(year))

trash_wheel_df = 
  bind_rows(MTW, PTW, GTW) |>
  relocate(category) 
```

*   The number of observations in the resulting datasets is as follows, where `trash_wheel_df` represents the final merged dataset:

|                | Mr. Trash Wheel  | Professor Trash Wheel | Gwynnda Trash Wheel | trash_wheel_df   |
|:--------------:|:----------------:|:---------------------:|:-------------------:|:----------------:|
| **observation**        | `r nrow(MTW)`     | `r nrow(PTW)`         | `r nrow(GTW)`       | `r nrow(trash_wheel_df)` |
| **variable**     | `r ncol(MTW)`     | `r ncol(PTW)`         | `r ncol(GTW)`       | `r ncol(trash_wheel_df)` |

Compared to **Mr. Trash Wheel**, **Professor Trash Wheel** and **Gwynnda Trash Wheel** are missing `sports_balls` and `glass_bottles` & `sports_balls`, respectively. These missing values result in a large number of "NA" in the **trash_wheel_df**. But they are still valid data that represents their own category, so **trash_wheel_df** is tidy. In addition, only `sports_balls` and `homes_powered` trash exist as multiple decimal places, while the values of other garbage types are integers.





```{r }
# Total weight of trash collected by Professor Trash Wheel
PTW_weight = PTW |>
  summarise(total_weight = sum(weight_tons, na.rm = TRUE))

# Total number of cigarette butts collected by Gwynnda in June 2022
GTW_cigarette = GTW |>
  filter(month(date) == 6, 
         year(date) == 2022) |>
  summarise(total_cigarette_butts = sum(cigarette_butts, na.rm = TRUE))
```

*   The total weight of trash collected by Professor Trash Wheel is `r PTW_weight`.

*   The total number of cigarette butts collected by Gwynnda in June of 2022 is `r GTW_cigarette`.

## III. Problem 3

### 3.1 Load all 4 csv

**bakers_df:**

*    Load bakers.csv;

*    Split the player's first name from `bakers_name` as `baker` so that it can be used as a key for later dataset merging;

*    Ensure there are no duplicate bakers;
```{r}
bakers_df = read.csv("./gbb_datasets/bakers.csv", 
                     na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  mutate(baker = sub(" .*", " ", baker_name)) |>
  mutate(baker = iconv(baker, from = "latin1", to = "UTF-8", sub = "")) |>
  mutate(baker = trimws(baker)) |> # remove " "
  distinct() |>
  arrange(baker) |>
  relocate(baker)
```

**bakes_df:**

*    Load bakes.csv;

*    Ensure there are no duplicate bakers;

*    Noticed that the name format of the player `"Jo"` is inconsistent with that of other players, since the double quotation marks are added. Modify it with `casematch`.
```{r}
bakes_df = read.csv("./gbb_datasets/bakes.csv", 
                     na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  distinct() |>
  mutate(baker = case_match(
    baker,
    '"Jo"' ~ "Jo",
    .default = baker)
    ) |> #keep other values unchanged
  mutate(baker = iconv(baker, from = "latin1", to = "UTF-8", sub = "")) |>
  mutate(baker = trimws(baker)) |>  # remove " "
  arrange(baker) |>
  relocate(baker)
```


**results_df:**

*    Load results.csv;

```{r}
results_df = read.csv("./gbb_datasets/results.csv", skip = 2,
                      na = c("NA", ".", "")) |>
  mutate(baker = iconv(baker, from = "latin1", to = "UTF-8", sub = "")) |>
  mutate(baker = trimws(baker)) |>  # remove " "
  janitor::clean_names() 
```


### 3.2 Check the Completeness
*    Identify if any baker in `results_df` is missing from the `bakers_df`.
```{r}
#anti_join(x, y, by = "key")  x have while y donot have
missing_bakers = anti_join(results_df, bakers_df, by = "baker") 

missing_bakers
```
The results show that Joanne's series 2 episodes 1-6 is present in the `results_df`, but not in the `bakers_df`.

*   Identify if any baker's bake in `results_df` is missing from the `bakes_df`.
```{r}
missing_bakes = anti_join(results_df, bakes_df, by = c("baker", "episode")) 
summary(missing_bakes)
```
The results shows that `r n_distinct(missing_bakes$baker)` bakers' bakes are missing from the `bakes_df`.

### 3.3 Merge Datasets
```{r}
# Merge all 3 datasets
combined_df = 
  results_df |>
  left_join(bakers_df, by = c("baker", "series")) |>
  left_join(bakes_df, by = c("baker", "series", "episode"))

# Reorganize the variables to be meaningful
final_df =
  combined_df |>
  select(series, episode, baker_name, technical, result, signature_bake, show_stopper, 
         baker_age, hometown, baker_occupation) |>
  arrange(series, episode,technical)

# export the final_df as csv
write_csv(final_df, "./gbb_datasets/Great British Bake Off.csv")
```
The final dataset `final_df` has `r nrow(final_df)` observations and `r ncol(final_df)` variables. 

In line with the preferences of viewers, this article places The Show's `series` and `eposide` at the top of the dataset, followed by `Bakers' Name` and their `technical`. The following are personal characteristics and background information about each baker, including `signature bake`, `show stopper bake`, `age`, `hometown`, and `occupation status`. 

### 3.4 Star Bakers

Filter results for Seasons 5 to 10 and select Star Baker.
```{r}
star_baker_df = results_df |>
  filter(series >= 5 & series <= 10, result %in% c("STAR BAKER", "WINNER")) |>
  select(series, episode, baker, result) |>
  mutate(result = case_match(
    result,
    "WINNER" ~ "STAR BAKER",
    .default = result)
  ) 
```

Create a table to show star bakers in Season 5 to 10, organizing by series and episode.
```{r}
library(knitr)
library(kableExtra)
star_baker_df |>
  arrange(series, episode) |>
  kable(caption = "Star Baker and Winners for Seasons 5 to 10",
        booktabs = TRUE) |>
  kable_styling() |>
  row_spec(which(star_baker_df$series %in% c(5, 7, 9)), background = "lightgray")
```

```{r}
baker_frequency = star_baker_df |>
  count(baker, sort = TRUE)  # sort = TRUE means descending
head(baker_frequency)
```

*    **Predictable Overall Winners:** *Richard Burr* won STAR BAKER 5 times, which is the most of all bakers. *Candice Brown*, *Nadiya Hussain* and *Steph Blackwell* won 4 times. Additionally, *Richard Burr* from series 5, *Ian Cumming* and *Nadiya Hussain* from series 6, *Steph Blackwell* from series 10 all consistently achieved STAR BAKER during their own series. To summarize, **Richard Burr** is the most predictable overall winners.

*    **Surprises:** It is surprising that **David Atherton** from series 10 was crowned STAR BAKER in episode 10, even though he did not won anyone before. 

### 3.5 viewers_df
Start by importing viewers.csv.
```{r}
viewers_df = read.csv("./gbb_datasets/viewers.csv", 
                      na = c("NA", ".", "")) |>
  janitor::clean_names() 

head(viewers_df, 10)

average_series_1 = viewers_df |>
  summarize(average = mean(series_1, na.rm = TRUE))
average_series_5 = viewers_df |>
  summarize(average = mean(series_5, na.rm = TRUE))

```
The average viewership in Season 1 is `r average_series_1`. In Season 5 is `r average_series_5`.
